{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-30T19:09:45.202846200Z",
     "start_time": "2025-10-30T19:09:45.197270400Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "\n",
    "# Variables used\n",
    "pdf_path = r\"C:\\Users\\pauca\\OneDrive\\Escritorio\\Personal\\CVs and Cover Letters\\Actual applications\\PauCaseCVTest.pdf\"\n",
    "\n",
    "first_agent = \"You will be given a candidate's CV, including summary bullet points, work experience, projects, skills and signature strengths and honors or awards. You are in charge of listing them in a clear format for another agent to understand.\"\n",
    "\n",
    "job_posting_agent = \"You will be given a job posting. Reduce the number of tokens while keepint ALL of the requirements, skills and any other information to the next prompted agent, who will receive your input to choose what parts of a cv match the job description best.\"\n",
    "\n",
    "second_agent_enhanced = \"You will receive a summary of a CV made by an AI agent and a job posting summary done by an another ai agent. First, process the job posting and then identify the most relevant 1) summary bullet points, 2) projects, 3) work experiences, 4) skills and signature strengths and 5) honors or awards that could fit in a one page CV. Take into consideration work experiences should be at most 1 or 2 and projects should be between 2-3.\"\n",
    "second_agent_equal = \"You will receive a summary of a CV made by an AI agent and a job posting summary done by an another ai agent. First, process the job posting and then identify the most relevant 1) summary bullet points, 2) projects, 3) work experiences, 4) skills and signature strengths and 5) honors or awards that could fit in a one page CV. Take into consideration work experiences should be at most 1 or 2 and projects should be between 2-3. You must not change any of the given candidate's inputs\"\n",
    "\n",
    "final_agent = \"You will receive parts of a CV made by an AI agent and you need to display the CV to be converted into a Word document easily programatically\"\n",
    "\n",
    "forLater=\"Then, you will get the content of a job posting, and you should match 3 projects and as many skills that match the job of the ones given.\"\n",
    "# Functions and agent\n",
    "def extract_text_from_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def groq_prompt(system_input, user_input, key):\n",
    "    headers = {\"Authorization\": f\"Bearer {key}\"}\n",
    "    data = {\n",
    "        \"model\": \"openai/gpt-oss-20b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_input},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", \n",
    "     headers=headers, json=data)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def get_key():\n",
    "    with open(r\"C:\\Users\\pauca\\PycharmProjects\\cvAdapter\\key.txt\", \"r\") as f:\n",
    "        key = f.readline()\n",
    "    return key.strip()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t = extract_text_from_pdf(pdf_path)\n",
    "jobPosting=\"\"\"\n",
    "Qorvo’s Internship Program is designed for college students currently enrolled in an accredited Bachelor’s, Master’s, or PhD program. Qorvo offers real work experience, exposure to upper management, and the opportunity to pursue full-time opportunities, as available. \n",
    "\n",
    "Qorvo’s Internship Program offers:\n",
    "\n",
    "Challenging, skill-building assignments\n",
    "Mentoring and coaching from industry experts\n",
    "Launch & Learns and other learning opportunities\n",
    "Collaborative team-based work environment\n",
    "Networking and social events\n",
    "Final presentation to business leaders\n",
    " \n",
    "\n",
    "Qorvo’s Data Analytics Internships are offered in our High Performance Analog, Advanced Cellular, Connectivity and Sensors and Global Operations business groups. Specific projects and responsibilities will be determined based on the business needs at the time of the internship assignment.\n",
    "\n",
    "Position Overview:\n",
    "We are seeking a motivated and detail-oriented rising Junior to join our Trade Compliance department for Summer 2026. The ideal candidate will be pursuing a major in Data Analytics, with a minor in Supply Chain Management or a related field. Other relevant majors may include Business Analytics, Information Systems, or Industrial Engineering.\n",
    "\n",
    "About Us:\n",
    "Our Global Trade Compliance team is a key driver of our international success, ensuring strict adherence to trade regulations across multiple regions. This team plays a strategic role in safeguarding our global operations while enabling business growth through data-driven compliance initiatives and cross-functional collaboration.\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Assist in analyzing trade compliance data to support ongoing audits and regulatory reporting.\n",
    "Contribute to the enhancement of current Key Performance Indicator (KPI) tracking and reporting processes.\n",
    "Support the development and automation of dashboards and tools for compliance metrics.\n",
    "Participate in projects aimed at improving import/export documentation workflows.\n",
    "Help identify trends and anomalies in trade data to support risk mitigation strategies.\n",
    "Collaborate with cross-functional teams including logistics, procurement, and IT to streamline compliance operations.\n",
    "Conduct research on trade regulations and assist in updating internal documentation.\n",
    "Qualifications:\n",
    "\n",
    "Rising Junior pursuing a Bachelor’s degree in Data Analytics, Supply Chain Management, Business Analytics, Information Systems, or Industrial Engineering.\n",
    "Strong analytical and problem-solving skills.\n",
    "Proficiency in Excel; familiarity with data visualization tools (e.g., Power BI, Tableau) is a plus.\n",
    "Excellent communication and organizational skills.\n",
    "Interest in international trade, compliance, and data-driven decision-making.\"\"\"\n",
    "\n",
    "#cvSummary = groq_prompt(first_agent, t, get_key())\n",
    "#jobSummary = groq_prompt(job_posting_agent, jobPosting,get_key())\n",
    "# newCvSummaryEnhanced = groq_prompt(second_agent_enhanced, cvSummary + \"\\nJob Posting details:\\n\" + jobSummary, get_key()) \n",
    "newCvSummary_equal = groq_prompt(second_agent_equal, cvSummary + \"\\nJob Posting details:\\n\" + jobSummary, get_key()) \n",
    "\n",
    "finalWordCV = groq_prompt(final_agent, newCvSummary_equal, get_key())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-30T19:09:57.385618800Z",
     "start_time": "2025-10-30T19:09:46.797239500Z"
    }
   },
   "id": "dfeeaa8c7a96cdb4",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent 1 - parsing content and understandably giving it to next agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c7250b850bcc7b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a **fully‑structured, Markdown‑ready CV** for **Pau Café Barrera**.  \n",
      "The format can be pasted into a Markdown editor or converted to Word (e.g., `pandoc -s input.md -o output.docx`) with minimal effort.\n",
      "\n",
      "---\n",
      "\n",
      "# Pau Casé Barrera\n",
      "\n",
      "## Relevant Summary\n",
      "- Eager learner with strong curiosity and analytical mindset  \n",
      "- Passionate about data‑driven solutions to real‑world problems  \n",
      "- Hands‑on experience with Tableau dashboarding & Python visualisation  \n",
      "- Skilled in statistical analysis, reporting, and database design  \n",
      "- Proficient in Agile methods & software design patterns  \n",
      "\n",
      "## Projects (2–3 most relevant)\n",
      "\n",
      "| # | Project | Why it matters for Trade‑Compliance Analytics | Key Tech |\n",
      "|---|---------|-----------------------------------------------|----------|\n",
      "| 1 | **Soccer Database Creation** – Oracle SQL; schema design, population, querying | Demonstrates strong SQL/database design skills and ability to build robust data models—critical for managing import/export data and audit trails. | Oracle SQL, SQL scripting |\n",
      "| 2 | **Credit Card Fraud Prediction** – Data prep, feature importance, ML pipeline in scikit‑learn | Showcases end‑to‑end data‑pipeline development, anomaly detection, and reporting—all useful for trend & risk analysis in trade compliance. | pandas, NumPy, scikit‑learn, Python |\n",
      "| 3 (optional) | **EEG Research** – Time‑series analysis of P300 signals | Highlights time‑series analytics and statistical rigor, helpful when parsing shipment timelines or customs windows. | Matplotlib, seaborn, scikit‑learn |\n",
      "\n",
      "## Work Experience (1–2 most relevant)\n",
      "\n",
      "| Role | Company | Dates | Key Responsibilities & Outcomes |\n",
      "|------|---------|-------|----------------------------------|\n",
      "| **Teacher & Team Lead** | Codelearn, Manresa | Sep 2023 – Sep 2025 | • Delivered technical instruction to students & families<br>• Led a team of teachers for 100+ students across 4 languages<br>• Planned curriculum based on attendance & trend analytics – *directly applies KPI tracking, reporting, and data‑driven decision‑making* |\n",
      "\n",
      "*(Teaching Assistant – Computer Graphics is excluded as it is less aligned with analytics and compliance.)*\n",
      "\n",
      "## Skills & Signature Strengths\n",
      "\n",
      "- **Data Analysis & Interpretation**  \n",
      "- **Data Visualization** (Tableau, Python)  \n",
      "- **Data Mining & Statistical Analysis**  \n",
      "- **KPI Tracking & Reporting**  \n",
      "- **Database Design & SQL**  \n",
      "- **Python** (pandas, NumPy, scikit‑learn)  \n",
      "- **Tableau**  \n",
      "- **Jupyter Notebook**  \n",
      "- **Agile Methodology & Collaborative Development**  \n",
      "- **Communication & Stakeholder Collaboration**  \n",
      "\n",
      "## Honors & Awards\n",
      "\n",
      "- Distinguished Delegation Award – National Model UN (NMUN), 2024  \n",
      "- LaSallian Master’s Scholarship, 2025‑Present  \n",
      "- LaSallian Bachelor’s Scholarship, 2022‑Present  \n",
      "- 13th/21 000 – Catalan Society of Mathematics, 2019  \n",
      "\n",
      "---\n",
      "\n",
      "**How to convert to Word programmatically**\n",
      "\n",
      "```bash\n",
      "# Install pandoc if you don't have it\n",
      "# On macOS: brew install pandoc\n",
      "# On Linux (Debian/Ubuntu): sudo apt-get install pandoc\n",
      "\n",
      "pandoc -s input.md -o Pau_Case_Barrera.docx\n",
      "```\n",
      "\n",
      "The resulting `.docx` file will preserve headings, tables, bullet lists, and line breaks exactly as shown.\n"
     ]
    }
   ],
   "source": [
    "# text = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "#clean = newCvSummary \n",
    "text = finalWordCV.replace('\\u202f', ' ')\n",
    "\n",
    "print(text)\n",
    "\n",
    "with open(\"test.md\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# newCvSummary\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-30T19:17:49.487127300Z",
     "start_time": "2025-10-30T19:17:49.457434900Z"
    }
   },
   "id": "336b17a1f924d2a2",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "435f7a522eabbc28"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2b95dd653ffea89"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypandoc\n",
      "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: pypandoc\n",
      "Successfully installed pypandoc-1.15\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-30T19:15:23.329329700Z",
     "start_time": "2025-10-30T19:15:16.932688400Z"
    }
   },
   "id": "9a7053f0e3c6592a",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No pandoc was found: either install pandoc and add it\nto your PATH or or call pypandoc.download_pandoc(...) or\ninstall pypandoc wheels with included pandoc.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpypandoc\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m pypandoc\u001B[38;5;241m.\u001B[39mconvert_text(\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest.md\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread(),\n\u001B[0;32m      5\u001B[0m     to\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocx\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmd\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m     outputfile\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput.docx\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      8\u001B[0m     extra_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--standalone\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      9\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pypandoc\\__init__.py:94\u001B[0m, in \u001B[0;36mconvert_text\u001B[1;34m(source, to, format, extra_args, encoding, outputfile, filters, verify_format, sandbox, cworkdir)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Converts given `source` from `format` to `to`.\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m:param str source: Unicode string or bytes (see encoding)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;124;03m        path.\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     93\u001B[0m source \u001B[38;5;241m=\u001B[39m _as_unicode(source, encoding)\n\u001B[1;32m---> 94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _convert_input(source, \u001B[38;5;28mformat\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m'\u001B[39m, to, extra_args\u001B[38;5;241m=\u001B[39mextra_args,\n\u001B[0;32m     95\u001B[0m                       outputfile\u001B[38;5;241m=\u001B[39moutputfile, filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[0;32m     96\u001B[0m                       verify_format\u001B[38;5;241m=\u001B[39mverify_format, sandbox\u001B[38;5;241m=\u001B[39msandbox,\n\u001B[0;32m     97\u001B[0m                       cworkdir\u001B[38;5;241m=\u001B[39mcworkdir)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pypandoc\\__init__.py:367\u001B[0m, in \u001B[0;36m_convert_input\u001B[1;34m(source, format, input_type, to, extra_args, outputfile, filters, verify_format, sandbox, cworkdir, sort_files)\u001B[0m\n\u001B[0;32m    364\u001B[0m _check_log_handler()\n\u001B[0;32m    366\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnsuring pandoc path...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 367\u001B[0m _ensure_pandoc_path()\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_format:\n\u001B[0;32m    370\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVerifying format...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pypandoc\\__init__.py:802\u001B[0m, in \u001B[0;36m_ensure_pandoc_path\u001B[1;34m()\u001B[0m\n\u001B[0;32m    794\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;124m    See http://johnmacfarlane.net/pandoc/installing.html\u001B[39m\n\u001B[0;32m    796\u001B[0m \u001B[38;5;124m    for installation options\u001B[39m\n\u001B[0;32m    797\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m))\n\u001B[0;32m    798\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m    799\u001B[0m \u001B[38;5;124m    ---------------------------------------------------------------\u001B[39m\n\u001B[0;32m    800\u001B[0m \n\u001B[0;32m    801\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m))\n\u001B[1;32m--> 802\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo pandoc was found: either install pandoc and add it\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    803\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto your PATH or or call pypandoc.download_pandoc(...) or\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    804\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstall pypandoc wheels with included pandoc.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: No pandoc was found: either install pandoc and add it\nto your PATH or or call pypandoc.download_pandoc(...) or\ninstall pypandoc wheels with included pandoc."
     ]
    }
   ],
   "source": [
    "import pypandoc\n",
    "\n",
    "pypandoc.convert_text(\n",
    "    open(\"test.md\").read(),\n",
    "    to=\"docx\",\n",
    "    format=\"md\",\n",
    "    outputfile=\"output.docx\",\n",
    "    extra_args=[\"--standalone\"]\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-30T19:19:27.084816500Z",
     "start_time": "2025-10-30T19:19:26.232081800Z"
    }
   },
   "id": "a89cc70ac2800650",
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
